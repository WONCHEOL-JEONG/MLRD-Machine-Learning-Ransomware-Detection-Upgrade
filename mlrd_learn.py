import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from sklearn import model_selection
from sklearn.metrics import f1_score, accuracy_score
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

def main():
    print('\n[+] Upgrading MLRD Machine Learning Model...')

    # ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€)
    try:
        df = pd.read_csv('data_file.csv', sep=',')
    except FileNotFoundError:
        print("âŒ Error: 'data_file.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ì„¸ìš”.")
        return

    # ë°ì´í„°ì…‹ ë¶„í¬ í™•ì¸ (ì¶”ê°€ëœ ì½”ë“œ)
    print("\n[+] Dataset Overview:")
    print(df['Benign'].value_counts())

    # íŠ¹ì§• ì„ íƒ ë° ë°ì´í„° ë¶„í• 
    X = df.drop(['FileName', 'md5Hash', 'Benign'], axis=1).values
    y = df['Benign'].values
    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)

    print("\n[*] Training samples:", len(X_train))
    print("[*] Testing samples:", len(X_test))

    # ëª¨ë¸ 1: ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ë°ì´í„° ë¶ˆê· í˜• í•´ê²° ì¶”ê°€)
    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    rf_clf.fit(X_train, y_train)
    rf_score = accuracy_score(y_test, rf_clf.predict(X_test))
    rf_f1 = f1_score(y_test, rf_clf.predict(X_test))

    # ëª¨ë¸ 2: XGBoost
    xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
    xgb_clf.fit(X_train, y_train)
    xgb_score = accuracy_score(y_test, xgb_clf.predict(X_test))
    xgb_f1 = f1_score(y_test, xgb_clf.predict(X_test))

    # ëª¨ë¸ 3: LightGBM
    lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)
    lgb_clf.fit(X_train, y_train)
    lgb_score = accuracy_score(y_test, lgb_clf.predict(X_test))
    lgb_f1 = f1_score(y_test, lgb_clf.predict(X_test))

    print("\n[*] Model Performance:")
    print(f"  - RandomForest Accuracy: {rf_score*100:.2f}%, F1-score: {rf_f1:.2f}")
    print(f"  - XGBoost Accuracy: {xgb_score*100:.2f}%, F1-score: {xgb_f1:.2f}")
    print(f"  - LightGBM Accuracy: {lgb_score*100:.2f}%, F1-score: {lgb_f1:.2f}")

    # ğŸ”¹ ìµœì  ëª¨ë¸ ìë™ ì„ íƒ
    best_model = max(
        [(rf_f1, rf_clf), (xgb_f1, xgb_clf), (lgb_f1, lgb_clf)], key=lambda x: x[0]
    )[1]

    print("\n[+] Selecting best model...")
    joblib.dump(best_model, 'classifier/best_model.pkl')
    print("[*] Model saved successfully.")

    # ğŸ”¹ íŠ¹ì§• ì„ íƒ ìˆ˜í–‰ (Feature Selection)
    print("\n[+] Performing feature selection...")
    feature_selector = SelectFromModel(best_model, threshold="median", prefit=True)
    X_selected = feature_selector.transform(X)

    print(f"[*] Reduced features from {X.shape[1]} to {X_selected.shape[1]}.")

if __name__ == '__main__':
    main()
